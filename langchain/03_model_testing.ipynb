{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Testing and Comparison\n\nThis notebook helps you test and compare different Ollama models to find the best one for your use case.\n\n**What you'll learn:**\n- How to test different models\n- Performance comparison techniques  \n- Model capability assessment\n- Choosing the right model for your task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\nimport sys\nsys.path.append('..')\n\nimport time\nimport requests\nfrom langchain_ollama import ChatOllama\nfrom langchain.schema import HumanMessage\nfrom config import config\n\nprint(\"üß™ Model Testing and Comparison\")\nprint(f\"Base URL: {config.ollama_base_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get available models\ndef get_available_models():\n    \"\"\"Get list of installed Ollama models.\"\"\"\n    try:\n        response = requests.get(f\"{config.ollama_base_url}/api/tags\")\n        if response.status_code == 200:\n            data = response.json()\n            return [model['name'] for model in data.get('models', [])]\n    except Exception as e:\n        print(f\"Error getting models: {e}\")\n    return []\n\navailable_models = get_available_models()\nprint(f\"Found {len(available_models)} available models:\")\nfor model in available_models:\n    print(f\"  - {model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test basic functionality of each model\ntest_prompt = \"Explain what Python is in one sentence.\"\n\ndef test_model(model_name, prompt, temperature=0.7):\n    \"\"\"Test a specific model with a prompt.\"\"\"\n    try:\n        llm = ChatOllama(\n            model=model_name,\n            base_url=config.ollama_base_url,\n            temperature=temperature\n        )\n        \n        start_time = time.time()\n        response = llm.invoke([HumanMessage(content=prompt)])\n        end_time = time.time()\n        \n        return {\n            'model': model_name,\n            'response': response.content,\n            'response_time': end_time - start_time,\n            'success': True\n        }\n    except Exception as e:\n        return {\n            'model': model_name,\n            'error': str(e),\n            'success': False\n        }\n\n# Test all available models\nprint(f\"",
    "Testing all models with prompt: '{test_prompt}'\")\nprint(\"=\" * 60)\n\nresults = []\nfor model in available_models:\n    print(f\"",
    "Testing {model}...\")\n    result = test_model(model, test_prompt)\n    results.append(result)\n    \n    if result['success']:\n        print(f\"‚úÖ Success ({result['response_time']:.2f}s)\")\n        print(f\"Response: {result['response'][:100]}...\")\n    else:\n        print(f\"‚ùå Failed: {result['error']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison\nsuccessful_results = [r for r in results if r['success']]\n\nif successful_results:\n    print(\"",
    "üìä Performance Summary:\")\n    print(\"=\" * 50)\n    \n    # Sort by response time\n    sorted_results = sorted(successful_results, key=lambda x: x['response_time'])\n    \n    for i, result in enumerate(sorted_results, 1):\n        print(f\"{i}. {result['model']}: {result['response_time']:.2f}s\")\n    \n    print(f\"",
    "üèÜ Fastest model: {sorted_results[0]['model']}\")\nelse:\n    print(\"‚ùå No models were successfully tested\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison Results\n\nBased on the tests above, you can see:\n- Which models are working correctly\n- Response times for each model\n- Quality of responses (review the truncated outputs)\n\n**Recommendations:**\n- Use faster models for simple tasks\n- Use more capable models for complex reasoning\n- Consider the trade-off between speed and quality"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}