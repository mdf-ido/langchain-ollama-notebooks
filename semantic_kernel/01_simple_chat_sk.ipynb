{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cffcb5e6",
   "metadata": {},
   "source": [
    "# Simple Chat with Semantic Kernel + Ollama\n",
    "\n",
    "This notebook demonstrates basic chat functionality using Semantic Kernel with Ollama.\n",
    "\n",
    "**What you'll learn:**\n",
    "- Semantic Kernel setup with Ollama\n",
    "- Basic chat interactions using SK\n",
    "- Chat history management\n",
    "- Kernel functions and plugins\n",
    "- Simple conversation flow with SK architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae0807b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import asyncio\n",
    "sys.path.append('..')\n",
    "\n",
    "# Semantic Kernel imports\n",
    "import semantic_kernel as sk\n",
    "from semantic_kernel.connectors.ai.ollama import OllamaChatCompletion\n",
    "from semantic_kernel.contents import ChatHistory\n",
    "from semantic_kernel.functions import kernel_function\n",
    "from semantic_kernel.functions.kernel_arguments import KernelArguments\n",
    "\n",
    "from config import config\n",
    "\n",
    "print(f\"Configuration loaded:\")\n",
    "print(f\"  Ollama URL: {config.ollama_base_url}\")\n",
    "print(f\"  Model: {config.model_name}\")\n",
    "print(f\"  Temperature: {config.temperature}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaed3531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Semantic Kernel with Ollama\n",
    "kernel = sk.Kernel()\n",
    "\n",
    "# Add Ollama chat completion service\n",
    "service_id = \"ollama_chat\"\n",
    "ollama_service = OllamaChatCompletion(\n",
    "    ai_model_id=config.model_name,\n",
    "    host=config.ollama_base_url,\n",
    "    service_id=service_id\n",
    ")\n",
    "\n",
    "kernel.add_service(ollama_service)\n",
    "\n",
    "print(\"✅ Semantic Kernel with Ollama configured successfully!\")\n",
    "print(f\"Service ID: {service_id}\")\n",
    "print(f\"Model: {config.model_name}\")\n",
    "print(f\"Ollama Host: {config.ollama_base_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f78fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple chat test using Semantic Kernel\n",
    "import os\n",
    "from semantic_kernel.connectors.ai.prompt_execution_settings import PromptExecutionSettings\n",
    "\n",
    "# Set environment variables as fallback\n",
    "os.environ[\"OLLAMA_HOST\"] = config.ollama_base_url\n",
    "print(f\"Set OLLAMA_HOST environment variable: {os.environ.get('OLLAMA_HOST')}\")\n",
    "\n",
    "async def simple_chat_test():\n",
    "    try:\n",
    "        print(\"Sending test message...\")\n",
    "        \n",
    "        # Create chat history and add a message\n",
    "        chat_history = ChatHistory()\n",
    "        chat_history.add_user_message(\"Hello! Please respond with a brief greeting.\")\n",
    "        \n",
    "        # Get the chat completion service\n",
    "        chat_service = kernel.get_service(type=OllamaChatCompletion)\n",
    "        \n",
    "        # Debug: Check if we can inspect the service configuration\n",
    "        print(f\"Chat service type: {type(chat_service)}\")\n",
    "        \n",
    "        # Create prompt execution settings\n",
    "        settings = PromptExecutionSettings(\n",
    "            max_tokens=500,\n",
    "            temperature=config.temperature\n",
    "        )\n",
    "        \n",
    "        # Get response\n",
    "        response = await chat_service.get_chat_message_contents(\n",
    "            chat_history=chat_history,\n",
    "            settings=settings\n",
    "        )\n",
    "        \n",
    "        print(f\"Response: {response[0].content}\")\n",
    "        print(\"✅ Chat is working!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {e}\")\n",
    "        print(\"Debugging info:\")\n",
    "        print(f\"  Config URL: {config.ollama_base_url}\")\n",
    "        print(f\"  OLLAMA_HOST env: {os.environ.get('OLLAMA_HOST')}\")\n",
    "        \n",
    "        # Test direct connection\n",
    "        try:\n",
    "            import requests\n",
    "            test_url = f\"{config.ollama_base_url}/api/tags\"\n",
    "            response = requests.get(test_url, timeout=5)\n",
    "            print(f\"  Direct API test: {response.status_code} - {test_url}\")\n",
    "        except Exception as conn_error:\n",
    "            print(f\"  Direct connection test failed: {conn_error}\")\n",
    "\n",
    "# Run the test\n",
    "await simple_chat_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de136fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple chat plugin using Semantic Kernel functions\n",
    "from semantic_kernel.functions import kernel_function\n",
    "\n",
    "class SimpleChatPlugin:\n",
    "    \"\"\"Simple chat plugin for Semantic Kernel.\"\"\"\n",
    "    \n",
    "    @kernel_function(\n",
    "        description=\"Have a conversation with the assistant\",\n",
    "        name=\"chat_with_context\"\n",
    "    )\n",
    "    async def chat_with_context(self, message: str, context: str = \"\") -> str:\n",
    "        \"\"\"Create a chat prompt with optional context.\"\"\"\n",
    "        \n",
    "        if context:\n",
    "            prompt = f\"\"\"You are a helpful assistant. \n",
    "\n",
    "Context: {context}\n",
    "\n",
    "User: {message}\n",
    "\n",
    "Please provide a helpful and relevant response.\"\"\"\n",
    "        else:\n",
    "            prompt = f\"\"\"You are a helpful assistant.\n",
    "\n",
    "User: {message}\n",
    "\n",
    "Please provide a helpful response.\"\"\"\n",
    "        \n",
    "        # Use the kernel's chat service directly\n",
    "        chat_service = kernel.get_service(type=OllamaChatCompletion)\n",
    "        chat_history = ChatHistory()\n",
    "        chat_history.add_user_message(prompt)\n",
    "        \n",
    "        settings = PromptExecutionSettings(\n",
    "            max_tokens=500,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        \n",
    "        response = await chat_service.get_chat_message_contents(\n",
    "            chat_history=chat_history,\n",
    "            settings=settings\n",
    "        )\n",
    "        \n",
    "        return response[0].content\n",
    "\n",
    "# Register the chat plugin\n",
    "chat_plugin = SimpleChatPlugin()\n",
    "kernel.add_plugin(chat_plugin, plugin_name=\"ChatPlugin\")\n",
    "\n",
    "print(\"✅ Chat plugin registered successfully!\")\n",
    "\n",
    "# Test the plugin using the correct invocation method\n",
    "async def test_chat_plugin():\n",
    "    try:\n",
    "        # Get the function from the plugin\n",
    "        chat_function = kernel.get_function(\"ChatPlugin\", \"chat_with_context\")\n",
    "        \n",
    "        # Invoke the function\n",
    "        result = await chat_function.invoke(\n",
    "            kernel,\n",
    "            KernelArguments(\n",
    "                message=\"What is Python used for?\",\n",
    "                context=\"The user is learning programming\"\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        print(f\"Plugin Response: {result.value}\")\n",
    "        print(\"✅ Plugin test successful!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Plugin error: {e}\")\n",
    "        \n",
    "        # Alternative: Direct method call\n",
    "        try:\n",
    "            print(\"Trying direct method call...\")\n",
    "            result = await chat_plugin.chat_with_context(\n",
    "                message=\"What is Python used for?\",\n",
    "                context=\"The user is learning programming\"\n",
    "            )\n",
    "            print(f\"Direct call result: {result}\")\n",
    "            print(\"✅ Direct call successful!\")\n",
    "        except Exception as e2:\n",
    "            print(f\"❌ Direct call error: {e2}\")\n",
    "\n",
    "await test_chat_plugin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60474580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversation with chat history management\n",
    "async def conversation_with_history():\n",
    "    print(\"\\n=== Conversation with Chat History ===\")\n",
    "    \n",
    "    # Initialize chat history with system message\n",
    "    chat_history = ChatHistory()\n",
    "    chat_history.add_system_message(\"You are a helpful programming assistant. Keep responses concise but informative.\")\n",
    "    \n",
    "    # Get chat service\n",
    "    chat_service = kernel.get_service(type=OllamaChatCompletion)\n",
    "    \n",
    "    # Simulate a conversation\n",
    "    conversation_turns = [\n",
    "        \"What is a Python list?\",\n",
    "        \"Can you show me an example of list comprehension?\",\n",
    "        \"How is that different from a regular for loop?\"\n",
    "    ]\n",
    "    \n",
    "    for i, user_message in enumerate(conversation_turns, 1):\n",
    "        print(f\"\\nTurn {i}:\")\n",
    "        print(f\"User: {user_message}\")\n",
    "        \n",
    "        # Add user message to history\n",
    "        chat_history.add_user_message(user_message)\n",
    "        \n",
    "        try:\n",
    "            # Get response using PromptExecutionSettings\n",
    "            response = await chat_service.get_chat_message_contents(\n",
    "                chat_history=chat_history,\n",
    "                settings=PromptExecutionSettings(\n",
    "                    max_tokens=800,\n",
    "                    temperature=0.7\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            assistant_response = response[0].content\n",
    "            print(f\"Assistant: {assistant_response}\")\n",
    "            \n",
    "            # Add assistant response to history\n",
    "            chat_history.add_assistant_message(assistant_response)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            break\n",
    "    \n",
    "    print(f\"\\n✅ Conversation completed with {len(chat_history.messages)} total messages\")\n",
    "\n",
    "await conversation_with_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0fbe4f",
   "metadata": {},
   "source": [
    "## Semantic Kernel vs LangChain Summary\n",
    "\n",
    "### Key Differences:\n",
    "\n",
    "**🧠 Semantic Kernel Approach:**\n",
    "- **Plugin Architecture**: Organize functions into reusable plugins\n",
    "- **Kernel Functions**: Use `@kernel_function` decorators for structured AI functions\n",
    "- **Built-in Chat History**: Native `ChatHistory` class for conversation management\n",
    "- **Service Architecture**: Clean separation between kernel, services, and plugins\n",
    "\n",
    "**⚡ LangChain Approach:**\n",
    "- **Chain Composition**: Build workflows through chain linking\n",
    "- **Direct Model Invocation**: Direct `llm.invoke()` calls\n",
    "- **Manual History Management**: Manual message array handling\n",
    "- **Extensive Ecosystem**: More integrations and community tools\n",
    "\n",
    "### When to Use Semantic Kernel:\n",
    "- Building structured AI applications with reusable components\n",
    "- Need strong plugin architecture and function organization  \n",
    "- Want built-in conversation and memory management\n",
    "- Prefer Microsoft's enterprise-focused approach\n",
    "\n",
    "### Troubleshooting Network Issues:\n",
    "If you encounter connection timeouts in JupyterHub/Kubernetes:\n",
    "\n",
    "```bash\n",
    "# Create External Service to bypass network policies\n",
    "kubectl apply -f - <<EOF\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: ollama-external\n",
    "spec:\n",
    "  type: ExternalName\n",
    "  externalName: 192.168.1.81\n",
    "  ports:\n",
    "  - port: 11434\n",
    "EOF\n",
    "```\n",
    "\n",
    "Then update config: `OLLAMA_BASE_URL=http://ollama-external:11434`"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
