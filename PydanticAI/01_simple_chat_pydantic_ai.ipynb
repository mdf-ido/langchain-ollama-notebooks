{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1273a220",
   "metadata": {},
   "source": [
    "# Simple Chat with PydanticAI\n",
    "\n",
    "This notebook demonstrates a basic chat interaction using PydanticAI framework with Ollama models.\n",
    "\n",
    "PydanticAI is a Python agent framework designed to make it easier to build production-grade applications with LLMs.\n",
    "It provides type-safe agent interactions, structured outputs, and dependency injection.\n",
    "\n",
    "## Setup Requirements\n",
    "- Ollama server running (configured in config.py)\n",
    "- PydanticAI installed (`pip install pydantic-ai`)\n",
    "- Model available in Ollama (e.g., 'gpt-oss', 'llama2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cd430a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "# Import shared configuration\n",
    "from config import config\n",
    "\n",
    "print(\"üìã Configuration loaded:\")\n",
    "print(f\"   Ollama URL: {config.ollama_base_url}\")\n",
    "print(f\"   Model: {config.model_name}\")\n",
    "print(f\"   Temperature: {config.temperature}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2af65c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Ollama connection before proceeding\n",
    "print(\"üîç Testing Ollama connection...\")\n",
    "\n",
    "if config.validate_ollama_connection():\n",
    "    print(\"‚úÖ Ollama server is accessible\")\n",
    "    \n",
    "    # Show available models\n",
    "    models = config.get_available_models()\n",
    "    if models:\n",
    "        print(f\"üì¶ Available models: {', '.join(models[:5])}...\")  # Show first 5\n",
    "        if config.model_name in models:\n",
    "            print(f\"‚úÖ Target model '{config.model_name}' is available\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  Target model '{config.model_name}' not found. Using first available.\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Could not retrieve model list\")\n",
    "else:\n",
    "    print(\"‚ùå Ollama server not accessible\")\n",
    "    print(f\"   Check if server is running at: {config.ollama_base_url}\")\n",
    "    print(\"   You may need to start Ollama or check network connectivity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73686206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PydanticAI\n",
    "try:\n",
    "    from pydantic_ai import Agent\n",
    "    from pydantic_ai.models import OllamaModel\n",
    "    print(\"‚úÖ PydanticAI imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(\"‚ùå PydanticAI not installed or import failed\")\n",
    "    print(\"   Install with: pip install pydantic-ai\")\n",
    "    print(f\"   Error: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ad3f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PydanticAI Agent with Ollama\n",
    "try:\n",
    "    # Create Ollama model connection\n",
    "    model = OllamaModel(\n",
    "        model_name=config.model_name,\n",
    "        base_url=config.ollama_base_url.rstrip('/'),  # Remove trailing slash if present\n",
    "    )\n",
    "    \n",
    "    # Create a simple chat agent\n",
    "    chat_agent = Agent(\n",
    "        model=model,\n",
    "        system_prompt=\"You are a helpful AI assistant. Respond clearly and concisely.\",\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ PydanticAI agent initialized successfully!\")\n",
    "    print(f\"   Model: {config.model_name}\")\n",
    "    print(f\"   Base URL: {config.ollama_base_url}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to initialize PydanticAI agent: {e}\")\n",
    "    print(\"\\nTroubleshooting:\")\n",
    "    print(\"1. Ensure Ollama server is running\")\n",
    "    print(\"2. Check if the model is available in Ollama\")\n",
    "    print(\"3. Verify network connectivity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3544495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple chat function\n",
    "async def simple_chat(message: str):\n",
    "    \"\"\"Send a message to the agent and get a response.\"\"\"\n",
    "    try:\n",
    "        result = await chat_agent.run(message)\n",
    "        return result.data\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "# Test the chat function\n",
    "import asyncio\n",
    "\n",
    "async def test_chat():\n",
    "    print(\"ü§ñ Testing simple chat...\")\n",
    "    \n",
    "    # Test message\n",
    "    test_message = \"Hello! Can you tell me what you are in one sentence?\"\n",
    "    print(f\"üë§ User: {test_message}\")\n",
    "    \n",
    "    response = await simple_chat(test_message)\n",
    "    print(f\"ü§ñ Assistant: {response}\")\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Run the test\n",
    "try:\n",
    "    if 'chat_agent' in locals():\n",
    "        response = await test_chat()\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Agent not initialized. Please run the previous cell first.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during chat test: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b43514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive chat loop\n",
    "async def interactive_chat():\n",
    "    \"\"\"Start an interactive chat session.\"\"\"\n",
    "    print(\"üí¨ Interactive Chat Started!\")\n",
    "    print(\"Type 'quit', 'exit', or 'bye' to end the conversation\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            # Get user input\n",
    "            user_input = input(\"üë§ You: \").strip()\n",
    "            \n",
    "            # Check for exit commands\n",
    "            if user_input.lower() in ['quit', 'exit', 'bye', '']:\n",
    "                print(\"üëã Goodbye!\")\n",
    "                break\n",
    "            \n",
    "            # Get AI response\n",
    "            print(\"ü§ñ Assistant: \", end=\"\", flush=True)\n",
    "            response = await simple_chat(user_input)\n",
    "            print(response)\n",
    "            print()\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nüëã Chat interrupted. Goodbye!\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {e}\")\n",
    "            print(\"Try again or type 'quit' to exit\")\n",
    "\n",
    "# Instructions for interactive chat\n",
    "print(\"üìù To start interactive chat, run:\")\n",
    "print(\"await interactive_chat()\")\n",
    "print(\"\\nNote: This works best in Jupyter environments that support async/await\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6dde50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Structured response with Pydantic models\n",
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "\n",
    "class ChatResponse(BaseModel):\n",
    "    \"\"\"Structured response model for chat interactions.\"\"\"\n",
    "    message: str\n",
    "    confidence: float\n",
    "    keywords: List[str]\n",
    "\n",
    "# Create an agent that returns structured data\n",
    "structured_agent = Agent(\n",
    "    model=model,\n",
    "    result_type=ChatResponse,\n",
    "    system_prompt=\"\"\"You are a helpful assistant that provides structured responses.\n",
    "    For each response, include:\n",
    "    - message: Your main response\n",
    "    - confidence: A confidence score between 0.0 and 1.0\n",
    "    - keywords: Key terms or concepts from the conversation\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "async def structured_chat(message: str):\n",
    "    \"\"\"Get a structured response from the agent.\"\"\"\n",
    "    try:\n",
    "        result = await structured_agent.run(message)\n",
    "        return result.data\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "# Test structured response\n",
    "async def test_structured_chat():\n",
    "    print(\"üîß Testing structured chat response...\")\n",
    "    \n",
    "    test_message = \"Explain what machine learning is in simple terms\"\n",
    "    print(f\"üë§ User: {test_message}\")\n",
    "    \n",
    "    response = await structured_chat(test_message)\n",
    "    \n",
    "    if isinstance(response, ChatResponse):\n",
    "        print(f\"ü§ñ Assistant: {response.message}\")\n",
    "        print(f\"üìä Confidence: {response.confidence}\")\n",
    "        print(f\"üè∑Ô∏è  Keywords: {', '.join(response.keywords)}\")\n",
    "    else:\n",
    "        print(f\"‚ùå {response}\")\n",
    "\n",
    "# Run structured test\n",
    "try:\n",
    "    if 'structured_agent' in locals():\n",
    "        await test_structured_chat()\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Structured agent not initialized. Please run the previous cell first.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during structured chat test: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f17f6c",
   "metadata": {},
   "source": [
    "## Key Features Demonstrated\n",
    "\n",
    "1. **Basic Chat**: Simple question-answer interaction\n",
    "2. **Structured Responses**: Using Pydantic models for type-safe outputs\n",
    "3. **Error Handling**: Robust error handling for network and model issues\n",
    "4. **Configuration Integration**: Uses shared config.py for consistent settings\n",
    "\n",
    "## PydanticAI vs Other Frameworks\n",
    "\n",
    "| Feature | PydanticAI | LangChain | Semantic Kernel |\n",
    "|---------|------------|-----------|----------------|\n",
    "| **Type Safety** | ‚úÖ Native Pydantic models | ‚ö†Ô∏è Manual typing | ‚ö†Ô∏è Limited typing |\n",
    "| **Structured Output** | ‚úÖ Built-in | üîß Requires setup | üîß Custom implementation |\n",
    "| **Async Support** | ‚úÖ Native async/await | ‚úÖ Supported | ‚úÖ Supported |\n",
    "| **Dependency Injection** | ‚úÖ Built-in | ‚ùå Manual | ‚ùå Manual |\n",
    "| **Validation** | ‚úÖ Automatic | üîß Manual | üîß Manual |\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Explore agent dependencies and dependency injection\n",
    "- Implement multi-agent conversations\n",
    "- Add function calling and tool integration\n",
    "- Create more complex structured response models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca2103e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup and summary\n",
    "print(\"üìã Summary:\")\n",
    "print(\"‚úÖ PydanticAI chat notebook completed\")\n",
    "print(\"üîß Features demonstrated:\")\n",
    "print(\"   - Basic chat interaction\")\n",
    "print(\"   - Structured responses with Pydantic models\")\n",
    "print(\"   - Error handling and connection validation\")\n",
    "print(\"   - Integration with shared configuration\")\n",
    "\n",
    "print(\"\\nüöÄ To extend this notebook:\")\n",
    "print(\"   - Add function calling capabilities\")\n",
    "print(\"   - Implement conversation memory\")\n",
    "print(\"   - Create multi-agent workflows\")\n",
    "print(\"   - Add streaming responses\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
