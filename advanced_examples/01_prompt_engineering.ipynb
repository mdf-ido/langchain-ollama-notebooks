{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Prompt Engineering Techniques\n",
    "\n",
    "This notebook demonstrates advanced prompt engineering techniques for better results with LangChain and Ollama.\n",
    "\n",
    "**What you'll learn:**\n",
    "- Advanced prompting patterns\n",
    "- Chain of thought reasoning\n",
    "- Role-based prompting\n",
    "- Prompt chaining and composition\n",
    "- Context management\n",
    "- Response validation and improvement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import sys\n",
    "sys.path.append('..')  # Add parent directory to path\n",
    "\n",
    "import json\n",
    "import re\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain.schema import SystemMessage, HumanMessage, AIMessage\n",
    "from config import config\n",
    "\n",
    "print(\"üéì Advanced Prompt Engineering with LangChain + Ollama\")\n",
    "print(f\"Using model: {config.default_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LLM with optimal settings for advanced techniques\n",
    "llm = ChatOllama(\n",
    "    model=config.default_model,\n",
    "    base_url=config.ollama_base_url,\n",
    "    temperature=0.3,  # Lower temperature for more consistent reasoning\n",
    "    max_tokens=1000   # Higher token limit for complex responses\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Advanced LLM configured successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Chain of Thought (CoT) Prompting\n",
    "print(\"üîó Chain of Thought Prompting...\")\n",
    "\n",
    "def create_cot_prompt(problem, domain=\"general\"):\n",
    "    \"\"\"Create a chain of thought prompt for complex problem solving.\"\"\"\n",
    "    \n",
    "    domain_contexts = {\n",
    "        \"math\": \"You are a mathematics expert who solves problems step by step.\",\n",
    "        \"programming\": \"You are a senior software engineer who analyzes code problems systematically.\",\n",
    "        \"analysis\": \"You are a data analyst who breaks down complex problems methodically.\",\n",
    "        \"general\": \"You are an expert problem solver who thinks step by step.\"\n",
    "    }\n",
    "    \n",
    "    template = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", domain_contexts.get(domain, domain_contexts[\"general\"])),\n",
    "        (\"human\", \"\"\"{problem}\n",
    "\n",
    "Please solve this step by step:\n",
    "1. First, clearly identify what we know and what we need to find\n",
    "2. Break down the problem into smaller parts\n",
    "3. Work through each part systematically\n",
    "4. Combine the results to reach the final answer\n",
    "5. Verify that your answer makes sense\n",
    "\n",
    "Show your reasoning at each step.\"\"\")\n",
    "    ])\n",
    "    \n",
    "    return template.format_messages(problem=problem)\n",
    "\n",
    "# Test CoT with a math problem\n",
    "math_problem = \"\"\"\n",
    "A company's revenue increased by 25% in Q1, decreased by 10% in Q2, \n",
    "and increased by 15% in Q3. If the Q3 revenue was $138,000, \n",
    "what was the original revenue at the start of Q1?\n",
    "\"\"\"\n",
    "\n",
    "cot_messages = create_cot_prompt(math_problem.strip(), \"math\")\n",
    "response = llm.invoke(cot_messages)\n",
    "\n",
    "print(f\"\\nüßÆ Chain of Thought - Math Problem:\")\n",
    "print(f\"Problem: {math_problem.strip()}\")\n",
    "print(f\"\\nü§ñ Step-by-step solution:\")\n",
    "print(response.content)\n",
    "print(\"\\n\" + \"-\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Role-Based Prompting with Personas\n",
    "print(\"üé≠ Role-Based Prompting with Detailed Personas...\")\n",
    "\n",
    "def create_persona_prompt(role, task, context=None):\n",
    "    \"\"\"Create a detailed persona-based prompt.\"\"\"\n",
    "    \n",
    "    personas = {\n",
    "        \"senior_developer\": {\n",
    "            \"identity\": \"You are a senior software developer with 10+ years of experience in Python, JavaScript, and system architecture.\",\n",
    "            \"style\": \"You provide practical, production-ready advice with code examples.\",\n",
    "            \"expertise\": \"You excel at debugging, optimization, and best practices.\"\n",
    "        },\n",
    "        \"data_scientist\": {\n",
    "            \"identity\": \"You are an experienced data scientist with expertise in machine learning, statistics, and data analysis.\",\n",
    "            \"style\": \"You explain complex concepts clearly and provide actionable insights.\",\n",
    "            \"expertise\": \"You specialize in Python, R, SQL, and various ML frameworks.\"\n",
    "        },\n",
    "        \"tech_writer\": {\n",
    "            \"identity\": \"You are a technical writer who specializes in making complex technical concepts accessible.\",\n",
    "            \"style\": \"You write clearly, use analogies, and structure information logically.\",\n",
    "            \"expertise\": \"You excel at documentation, tutorials, and explaining technical topics.\"\n",
    "        },\n",
    "        \"consultant\": {\n",
    "            \"identity\": \"You are a business consultant with deep technical knowledge and strategic thinking.\",\n",
    "            \"style\": \"You provide balanced analysis considering both technical and business aspects.\",\n",
    "            \"expertise\": \"You excel at requirement analysis, solution design, and risk assessment.\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    persona = personas.get(role, personas[\"consultant\"])\n",
    "    \n",
    "    system_prompt = f\"\"\"{persona['identity']}\n",
    "{persona['style']}\n",
    "{persona['expertise']}\n",
    "\n",
    "Context: {context if context else 'General consultation'}\n",
    "\n",
    "Always:\n",
    "- Stay in character\n",
    "- Provide specific, actionable advice\n",
    "- Use examples when helpful\n",
    "- Consider practical limitations\"\"\"\n",
    "    \n",
    "    template = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{task}\")\n",
    "    ])\n",
    "    \n",
    "    return template.format_messages(task=task)\n",
    "\n",
    "# Test with different personas\n",
    "task = \"I need to build a REST API for a small e-commerce application. What's the best approach?\"\n",
    "context = \"Small startup, 2-person team, need to launch quickly but plan for growth\"\n",
    "\n",
    "personas_to_test = [\"senior_developer\", \"consultant\"]\n",
    "\n",
    "for persona in personas_to_test:\n",
    "    messages = create_persona_prompt(persona, task, context)\n",
    "    response = llm.invoke(messages)\n",
    "    \n",
    "    print(f\"\\nüé≠ {persona.replace('_', ' ').title()} perspective:\")\n",
    "    print(f\"üìù {response.content[:300]}...\")\n",
    "    print()\n",
    "\n",
    "print(\"-\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Multi-Step Reasoning with Prompt Chaining\n",
    "print(\"‚õìÔ∏è Multi-Step Reasoning with Prompt Chaining...\")\n",
    "\n",
    "def analyze_code_quality(code_snippet):\n",
    "    \"\"\"Multi-step code analysis using prompt chaining.\"\"\"\n",
    "    \n",
    "    # Step 1: Initial analysis\n",
    "    analysis_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a code reviewer. Analyze the given code for functionality, readability, and potential issues.\"),\n",
    "        (\"human\", \"Analyze this Python code and identify: 1) What it does, 2) Potential issues, 3) Code quality aspects\\n\\nCode:\\n{code}\")\n",
    "    ])\n",
    "    \n",
    "    step1_messages = analysis_prompt.format_messages(code=code_snippet)\n",
    "    analysis_result = llm.invoke(step1_messages)\n",
    "    \n",
    "    # Step 2: Improvement suggestions based on analysis\n",
    "    improvement_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a senior developer providing code improvement suggestions.\"),\n",
    "        (\"human\", \"Based on this code analysis, provide specific improvement suggestions with examples:\\n\\nOriginal Code:\\n{code}\\n\\nAnalysis:\\n{analysis}\")\n",
    "    ])\n",
    "    \n",
    "    step2_messages = improvement_prompt.format_messages(code=code_snippet, analysis=analysis_result.content)\n",
    "    improvements = llm.invoke(step2_messages)\n",
    "    \n",
    "    # Step 3: Refactored version\n",
    "    refactor_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are an expert Python developer. Provide a refactored version of the code.\"),\n",
    "        (\"human\", \"Please refactor this code incorporating the suggested improvements:\\n\\nOriginal Code:\\n{code}\\n\\nSuggested Improvements:\\n{improvements}\")\n",
    "    ])\n",
    "    \n",
    "    step3_messages = refactor_prompt.format_messages(code=code_snippet, improvements=improvements.content)\n",
    "    refactored = llm.invoke(step3_messages)\n",
    "    \n",
    "    return {\n",
    "        \"analysis\": analysis_result.content,\n",
    "        \"improvements\": improvements.content,\n",
    "        \"refactored\": refactored.content\n",
    "    }\n",
    "\n",
    "# Test with a code snippet\n",
    "test_code = \"\"\"\n",
    "def calc(a, b, op):\n",
    "    if op == '+':\n",
    "        return a + b\n",
    "    elif op == '-':\n",
    "        return a - b\n",
    "    elif op == '*':\n",
    "        return a * b\n",
    "    elif op == '/':\n",
    "        return a / b\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "result = calc(10, 0, '/')\n",
    "print(result)\n",
    "\"\"\".strip()\n",
    "\n",
    "print(f\"\\nüîç Multi-step code analysis:\")\n",
    "print(f\"Original code:\\n{test_code}\")\n",
    "\n",
    "results = analyze_code_quality(test_code)\n",
    "\n",
    "print(f\"\\nüìä Step 1 - Analysis:\")\n",
    "print(results[\"analysis\"][:200] + \"...\")\n",
    "\n",
    "print(f\"\\nüí° Step 2 - Improvements:\")\n",
    "print(results[\"improvements\"][:200] + \"...\")\n",
    "\n",
    "print(f\"\\n‚ú® Step 3 - Refactored Code:\")\n",
    "print(results[\"refactored\"][:300] + \"...\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Context-Aware Conversation Management\n",
    "print(\"üí¨ Context-Aware Conversation Management...\")\n",
    "\n",
    "class ContextualChat:\n",
    "    \"\"\"Manages conversation context for better responses.\"\"\"\n",
    "    \n",
    "    def __init__(self, llm, system_prompt=None, max_history=10):\n",
    "        self.llm = llm\n",
    "        self.conversation_history = []\n",
    "        self.max_history = max_history\n",
    "        self.system_prompt = system_prompt or \"You are a helpful AI assistant with excellent memory and context awareness.\"\n",
    "    \n",
    "    def chat(self, user_input, context_summary=None):\n",
    "        \"\"\"Send a message with full conversation context.\"\"\"\n",
    "        \n",
    "        messages = [SystemMessage(content=self.system_prompt)]\n",
    "        \n",
    "        # Add context summary if conversation is getting long\n",
    "        if context_summary:\n",
    "            messages.append(SystemMessage(content=f\"Context summary: {context_summary}\"))\n",
    "        \n",
    "        # Add recent conversation history\n",
    "        messages.extend(self.conversation_history[-self.max_history:])\n",
    "        \n",
    "        # Add current user input\n",
    "        messages.append(HumanMessage(content=user_input))\n",
    "        \n",
    "        # Get response\n",
    "        response = self.llm.invoke(messages)\n",
    "        \n",
    "        # Update conversation history\n",
    "        self.conversation_history.append(HumanMessage(content=user_input))\n",
    "        self.conversation_history.append(AIMessage(content=response.content))\n",
    "        \n",
    "        return response.content\n",
    "    \n",
    "    def get_context_summary(self):\n",
    "        \"\"\"Generate a summary of the conversation for context management.\"\"\"\n",
    "        if len(self.conversation_history) < 4:\n",
    "            return None\n",
    "        \n",
    "        # Create summary prompt\n",
    "        history_text = \"\\n\".join([f\"{msg.__class__.__name__}: {msg.content}\" for msg in self.conversation_history])\n",
    "        \n",
    "        summary_prompt = f\"\"\"Summarize the key points and context from this conversation in 2-3 sentences:\n",
    "        \n",
    "{history_text}\"\"\"\n",
    "        \n",
    "        summary = self.llm.invoke(summary_prompt)\n",
    "        return summary.content\n",
    "\n",
    "# Test contextual conversation\n",
    "chat = ContextualChat(\n",
    "    llm, \n",
    "    system_prompt=\"You are a Python programming tutor. You remember what we've discussed and build on previous topics.\"\n",
    ")\n",
    "\n",
    "print(f\"\\nüí¨ Contextual Conversation Test:\")\n",
    "\n",
    "# Simulate a multi-turn conversation\n",
    "conversation = [\n",
    "    \"I'm learning Python and want to understand functions better.\",\n",
    "    \"Can you show me how to create a function with parameters?\",\n",
    "    \"What about default parameters in the function we just discussed?\",\n",
    "    \"How would I modify our function to handle errors?\"\n",
    "]\n",
    "\n",
    "for i, user_msg in enumerate(conversation, 1):\n",
    "    print(f\"\\nüë§ Turn {i}: {user_msg}\")\n",
    "    response = chat.chat(user_msg)\n",
    "    print(f\"ü§ñ Response: {response[:150]}...\")\n",
    "\n",
    "# Show context summary\n",
    "summary = chat.get_context_summary()\n",
    "print(f\"\\nüìã Conversation Summary: {summary[:100]}...\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Response Validation and Self-Correction\n",
    "print(\"‚úÖ Response Validation and Self-Correction...\")\n",
    "\n",
    "def validated_response(prompt, validation_criteria, max_attempts=3):\n",
    "    \"\"\"Generate a response and validate it against criteria.\"\"\"\n",
    "    \n",
    "    for attempt in range(max_attempts):\n",
    "        print(f\"  Attempt {attempt + 1}...\")\n",
    "        \n",
    "        # Generate response\n",
    "        response = llm.invoke(prompt)\n",
    "        content = response.content\n",
    "        \n",
    "        # Validate response\n",
    "        validation_prompt = f\"\"\"\n",
    "Please evaluate this response against the given criteria:\n",
    "\n",
    "Original Request: {prompt}\n",
    "Response: {content}\n",
    "Criteria: {validation_criteria}\n",
    "\n",
    "Does the response meet all criteria? Answer with:\n",
    "- \"YES\" if it meets all criteria\n",
    "- \"NO: [specific issues]\" if it doesn't\n",
    "\"\"\"\n",
    "        \n",
    "        validation = llm.invoke(validation_prompt)\n",
    "        validation_result = validation.content.strip()\n",
    "        \n",
    "        print(f\"    Validation: {validation_result[:100]}...\")\n",
    "        \n",
    "        if validation_result.upper().startswith(\"YES\"):\n",
    "            print(f\"  ‚úÖ Response validated successfully!\")\n",
    "            return {\n",
    "                \"response\": content,\n",
    "                \"attempts\": attempt + 1,\n",
    "                \"validated\": True,\n",
    "                \"validation_feedback\": validation_result\n",
    "            }\n",
    "        \n",
    "        # If not valid, modify the prompt for next attempt\n",
    "        if attempt < max_attempts - 1:\n",
    "            prompt = f\"\"\"{prompt}\n",
    "            \n",
    "Previous attempt didn't meet criteria: {validation_result}\n",
    "Please address these issues in your response.\"\"\"\n",
    "    \n",
    "    print(f\"  ‚ùå Failed to generate valid response after {max_attempts} attempts\")\n",
    "    return {\n",
    "        \"response\": content,\n",
    "        \"attempts\": max_attempts,\n",
    "        \"validated\": False,\n",
    "        \"validation_feedback\": validation_result\n",
    "    }\n",
    "\n",
    "# Test with validation\n",
    "test_prompt = \"Write a Python function to calculate fibonacci numbers.\"\n",
    "validation_criteria = \"\"\"\n",
    "1. Must include proper function definition with parameters\n",
    "2. Must include docstring explaining what the function does\n",
    "3. Must handle edge cases (n=0, n=1)\n",
    "4. Must include a simple usage example\n",
    "5. Code must be properly formatted\n",
    "\"\"\"\n",
    "\n",
    "print(f\"\\nüîç Testing response validation:\")\n",
    "print(f\"Request: {test_prompt}\")\n",
    "print(f\"Criteria: Must meet all 5 requirements\")\n",
    "\n",
    "result = validated_response(test_prompt, validation_criteria)\n",
    "\n",
    "print(f\"\\nüìä Results:\")\n",
    "print(f\"  Attempts: {result['attempts']}\")\n",
    "print(f\"  Validated: {result['validated']}\")\n",
    "print(f\"\\nüìù Final Response:\")\n",
    "print(result['response'][:400] + \"...\" if len(result['response']) > 400 else result['response'])\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Advanced Technique Summary and Best Practices\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ADVANCED PROMPT ENGINEERING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "techniques_summary = {\n",
    "    \"üîó Chain of Thought\": {\n",
    "        \"when\": \"Complex reasoning, math problems, multi-step analysis\",\n",
    "        \"tip\": \"Use explicit step-by-step instructions and ask for verification\"\n",
    "    },\n",
    "    \"üé≠ Role-Based Prompting\": {\n",
    "        \"when\": \"Domain-specific advice, different perspectives needed\",\n",
    "        \"tip\": \"Create detailed personas with expertise and communication style\"\n",
    "    },\n",
    "    \"‚õìÔ∏è Prompt Chaining\": {\n",
    "        \"when\": \"Multi-stage processing, complex analysis workflows\",\n",
    "        \"tip\": \"Break complex tasks into sequential, focused prompts\"\n",
    "    },\n",
    "    \"üí¨ Context Management\": {\n",
    "        \"when\": \"Long conversations, building on previous responses\",\n",
    "        \"tip\": \"Maintain context but summarize when history gets long\"\n",
    "    },\n",
    "    \"‚úÖ Response Validation\": {\n",
    "        \"when\": \"Quality assurance, specific requirements needed\",\n",
    "        \"tip\": \"Define clear criteria and iterate until satisfied\"\n",
    "    }\n",
    "}\n",
    "\n",
    "for technique, info in techniques_summary.items():\n",
    "    print(f\"\\n{technique}:\")\n",
    "    print(f\"  Use when: {info['when']}\")\n",
    "    print(f\"  Best practice: {info['tip']}\")\n",
    "\n",
    "print(f\"\\nüéØ Key Success Factors:\")\n",
    "success_factors = [\n",
    "    \"Clear, specific instructions\",\n",
    "    \"Appropriate temperature settings (0.1-0.3 for reasoning)\",\n",
    "    \"Sufficient token limits for complex responses\",\n",
    "    \"Iterative refinement based on results\",\n",
    "    \"Context awareness and memory management\",\n",
    "    \"Validation and quality checks\"\n",
    "]\n",
    "\n",
    "for factor in success_factors:\n",
    "    print(f\"  ‚Ä¢ {factor}\")\n",
    "\n",
    "print(f\"\\n‚ö° Performance Tips:\")\n",
    "performance_tips = [\n",
    "    \"Use lower temperatures (0.1-0.3) for analytical tasks\",\n",
    "    \"Increase max_tokens for complex multi-step responses\",\n",
    "    \"Cache persona definitions for reuse\",\n",
    "    \"Implement conversation summarization for long chats\",\n",
    "    \"Test prompts with different models to find best fit\",\n",
    "    \"Monitor token usage to optimize costs\"\n",
    "]\n",
    "\n",
    "for tip in performance_tips:\n",
    "    print(f\"  ‚Ä¢ {tip}\")\n",
    "\n",
    "print(f\"\\nüöÄ Next Steps:\")\n",
    "print(f\"‚Ä¢ Combine techniques for more sophisticated applications\")\n",
    "print(f\"‚Ä¢ Build prompt libraries for common use cases\")\n",
    "print(f\"‚Ä¢ Explore retrieval-augmented generation (RAG)\")\n",
    "print(f\"‚Ä¢ Next: advanced_examples/02_chains_and_workflows.ipynb\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  }
 ],\n "metadata": {\n  "kernelspec": {\n   "display_name": "Python 3",\n   "language": "python",\n   "name": "python3"\n  },\n  "language_info": {\n   "codemirror_mode": {\n    "name": "ipython",\n    "version": 3\n   },\n   "file_extension": ".py",\n   "mimetype": "text/x-python",\n   "name": "python",\n   "nbconvert_exporter": "python",\n   "pygments_lexer": "ipython3",\n   "version": "3.8.5"\n  }\n },\n "nbformat": 4,\n "nbformat_minor": 4\n}