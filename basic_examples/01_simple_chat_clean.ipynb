{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddfab30f",
   "metadata": {},
   "source": [
    "# Simple Chat with Ollama\n",
    "\n",
    "This notebook demonstrates basic chat functionality using LangChain and Ollama.\n",
    "\n",
    "## Prerequisites\n",
    "- Ollama server running\n",
    "- LangChain installed\n",
    "- Configuration set in config.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77a6b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import requests\n",
    "sys.path.append('..')\n",
    "from config import *\n",
    "from langchain_ollama import ChatOllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82085172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test connection to Ollama server\n",
    "print(f\"Testing connection to: {ollama_base_url}\")\n",
    "try:\n",
    "    response = requests.get(ollama_base_url, timeout=10)\n",
    "    if response.status_code == 200:\n",
    "        print(\"✅ Connection successful!\")\n",
    "    else:\n",
    "        print(f\"⚠️ Connection returned status: {response.status_code}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Connection failed: {e}\")\n",
    "    print(\"\\nIf you're in JupyterHub, this might be due to network policies.\")\n",
    "    print(\"Solution: Create a Kubernetes service to bypass the policy.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3709d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the chat model\n",
    "print(f\"Initializing model: {model_name}\")\n",
    "\n",
    "llm = ChatOllama(\n",
    "    base_url=ollama_base_url,\n",
    "    model=model_name,\n",
    "    temperature=temperature,\n",
    "    request_timeout=request_timeout,\n",
    "    timeout=connection_timeout\n",
    ")\n",
    "\n",
    "print(\"✅ Chat model initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2d9308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple chat test\n",
    "try:\n",
    "    print(\"Sending test message...\")\n",
    "    response = llm.invoke(\"Hello! Please respond with a brief greeting.\")\n",
    "    print(f\"\\nResponse: {response.content}\")\n",
    "    print(\"\\n✅ Chat is working successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: {e}\")\n",
    "    print(\"\\nTroubleshooting tips:\")\n",
    "    print(\"1. Check if Ollama server is running\")\n",
    "    print(\"2. Verify the model is available: ollama list\")\n",
    "    print(\"3. Check network connectivity\")\n",
    "    print(\"4. If in Kubernetes/JupyterHub, check network policies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22698283",
   "metadata": {},
   "source": [
    "## Troubleshooting Network Issues\n",
    "\n",
    "If you're experiencing connection timeouts in JupyterHub, create a Kubernetes service:\n",
    "\n",
    "```bash\n",
    "kubectl apply -f - <<EOF\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: ollama-external\n",
    "spec:\n",
    "  type: ExternalName\n",
    "  externalName: 192.168.1.81\n",
    "  ports:\n",
    "  - port: 11434\n",
    "EOF\n",
    "```\n",
    "\n",
    "Then set: `OLLAMA_BASE_URL=http://ollama-external:11434`"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
