{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Testing and Comparison\n",
    "\n",
    "This notebook helps you test and compare different Ollama models with LangChain.\n",
    "\n",
    "**What you'll learn:**\n",
    "- Testing multiple models programmatically\n",
    "- Performance benchmarking\n",
    "- Response quality comparison\n",
    "- Model selection strategies\n",
    "- Optimization techniques\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import sys\n",
    "sys.path.append('..')  # Add parent directory to path\n",
    "\n",
    "import time\n",
    "import requests\n",
    "from langchain_ollama import ChatOllama\n",
    "from config import config\n",
    "\n",
    "print(\"üß™ Setting up Model Testing and Comparison...\")\n",
    "print(f\"Ollama URL: {config.ollama_base_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Discover Available Models\n",
    "def get_available_models():\n",
    "    \"\"\"Get list of models available on the Ollama server.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(f\"{config.ollama_base_url}/api/tags\", timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            return data.get('models', [])\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting models: {e}\")\n",
    "        return []\n",
    "\n",
    "models = get_available_models()\n",
    "print(f\"\\nüì¶ Found {len(models)} available models:\")\n",
    "\n",
    "if models:\n",
    "    for i, model in enumerate(models, 1):\n",
    "        name = model['name']\n",
    "        size = model.get('size', 0) / (1024**3)  # Convert to GB\n",
    "        print(f\"  {i}. {name} ({size:.1f}GB)\")\n",
    "        \n",
    "    model_names = [model['name'] for model in models]\n",
    "else:\n",
    "    print(\"  No models found. Please install at least one model:\")\n",
    "    print(\"  ollama pull llama2\")\n",
    "    model_names = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Model Performance Testing\n",
    "def test_model_performance(model_name, test_prompts, temperature=0.7, max_tokens=200):\n",
    "    \"\"\"Test a model's performance with a set of prompts.\"\"\"\n",
    "    \n",
    "    print(f\"\\nüß™ Testing {model_name}...\")\n",
    "    \n",
    "    try:\n",
    "        llm = ChatOllama(\n",
    "            model=model_name,\n",
    "            base_url=config.ollama_base_url,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens\n",
    "        )\n",
    "        \n",
    "        results = {\n",
    "            'model': model_name,\n",
    "            'tests': [],\n",
    "            'total_time': 0,\n",
    "            'avg_time': 0,\n",
    "            'success_rate': 0\n",
    "        }\n",
    "        \n",
    "        successful_tests = 0\n",
    "        \n",
    "        for i, prompt in enumerate(test_prompts, 1):\n",
    "            print(f\"  Test {i}/{len(test_prompts)}: {prompt[:50]}...\")\n",
    "            \n",
    "            try:\n",
    "                start_time = time.time()\n",
    "                response = llm.invoke(prompt)\n",
    "                end_time = time.time()\n",
    "                \n",
    "                test_time = end_time - start_time\n",
    "                results['total_time'] += test_time\n",
    "                \n",
    "                test_result = {\n",
    "                    'prompt': prompt,\n",
    "                    'response': response.content,\n",
    "                    'time': test_time,\n",
    "                    'success': True,\n",
    "                    'response_length': len(response.content)\n",
    "                }\n",
    "                \n",
    "                results['tests'].append(test_result)\n",
    "                successful_tests += 1\n",
    "                \n",
    "                print(f\"    ‚úÖ {test_time:.2f}s | {len(response.content)} chars\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"    ‚ùå Error: {e}\")\n",
    "                test_result = {\n",
    "                    'prompt': prompt,\n",
    "                    'response': None,\n",
    "                    'time': 0,\n",
    "                    'success': False,\n",
    "                    'error': str(e)\n",
    "                }\n",
    "                results['tests'].append(test_result)\n",
    "        \n",
    "        results['avg_time'] = results['total_time'] / len(test_prompts) if test_prompts else 0\n",
    "        results['success_rate'] = (successful_tests / len(test_prompts)) * 100 if test_prompts else 0\n",
    "        \n",
    "        print(f\"  üìä Summary: {successful_tests}/{len(test_prompts)} successful, avg {results['avg_time']:.2f}s\")\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Failed to initialize model: {e}\")\n",
    "        return None\n",
    "\n",
    "# Define test prompts\n",
    "test_prompts = [\n",
    "    \"What is Python?\",\n",
    "    \"Explain machine learning in one sentence.\",\n",
    "    \"Write a simple 'Hello, World!' program in Python.\",\n",
    "    \"List three benefits of using version control.\",\n",
    "    \"What's 15 * 24?\"\n",
    "]\n",
    "\n",
    "print(f\"\\nüéØ Test prompts prepared ({len(test_prompts)} prompts)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Run Tests on Available Models\n",
    "all_results = []\n",
    "\n",
    "if model_names:\n",
    "    print(\"\\nüöÄ Running performance tests...\")\n",
    "    \n",
    "    # Test up to 3 models to keep execution time reasonable\n",
    "    models_to_test = model_names[:3]\n",
    "    \n",
    "    for model_name in models_to_test:\n",
    "        result = test_model_performance(model_name, test_prompts)\n",
    "        if result:\n",
    "            all_results.append(result)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No models available for testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Performance Comparison\n",
    "if all_results:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PERFORMANCE COMPARISON\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Sort by average response time\n",
    "    all_results.sort(key=lambda x: x['avg_time'])\n",
    "    \n",
    "    print(f\"{'Model':<20} {'Avg Time':<10} {'Success Rate':<12} {'Total Time':<12}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for result in all_results:\n",
    "        model = result['model'][:18]\n",
    "        avg_time = f\"{result['avg_time']:.2f}s\"\n",
    "        success_rate = f\"{result['success_rate']:.1f}%\"\n",
    "        total_time = f\"{result['total_time']:.2f}s\"\n",
    "        \n",
    "        print(f\"{model:<20} {avg_time:<10} {success_rate:<12} {total_time:<12}\")\n",
    "    \n",
    "    # Find best performers\n",
    "    fastest_model = min(all_results, key=lambda x: x['avg_time'])\n",
    "    most_reliable = max(all_results, key=lambda x: x['success_rate'])\n",
    "    \n",
    "    print(f\"\\nüèÜ Fastest model: {fastest_model['model']} ({fastest_model['avg_time']:.2f}s avg)\")\n",
    "    print(f\"üéØ Most reliable: {most_reliable['model']} ({most_reliable['success_rate']:.1f}% success)\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No test results to compare\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Response Quality Comparison\n",
    "if all_results:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RESPONSE QUALITY COMPARISON\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Compare responses to a specific prompt\n",
    "    comparison_prompt = \"Explain machine learning in one sentence.\"\n",
    "    \n",
    "    print(f\"\\nüìù Comparing responses to: '{comparison_prompt}'\\n\")\n",
    "    \n",
    "    for result in all_results:\n",
    "        model_name = result['model']\n",
    "        \n",
    "        # Find the test with our comparison prompt\n",
    "        for test in result['tests']:\n",
    "            if test['prompt'] == comparison_prompt and test['success']:\n",
    "                response_length = len(test['response'])\n",
    "                response_time = test['time']\n",
    "                \n",
    "                print(f\"ü§ñ {model_name} ({response_time:.2f}s, {response_length} chars):\")\n",
    "                print(f\"   {test['response'][:200]}{'...' if len(test['response']) > 200 else ''}\")\n",
    "                print()\n",
    "                break\n",
    "        else:\n",
    "            print(f\"ü§ñ {model_name}: No successful response to this prompt\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Temperature Testing\n",
    "def test_temperature_effects(model_name, prompt, temperatures=[0.0, 0.5, 1.0]):\n",
    "    \"\"\"Test how temperature affects model responses.\"\"\"\n",
    "    \n",
    "    print(f\"\\nüå°Ô∏è Testing temperature effects on {model_name}...\")\n",
    "    print(f\"Prompt: '{prompt}'\\n\")\n",
    "    \n",
    "    for temp in temperatures:\n",
    "        try:\n",
    "            llm = ChatOllama(\n",
    "                model=model_name,\n",
    "                base_url=config.ollama_base_url,\n",
    "                temperature=temp,\n",
    "                max_tokens=100\n",
    "            )\n",
    "            \n",
    "            start_time = time.time()\n",
    "            response = llm.invoke(prompt)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            print(f\"üå°Ô∏è Temperature {temp} ({end_time-start_time:.2f}s):\")\n",
    "            print(f\"   {response.content[:150]}{'...' if len(response.content) > 150 else ''}\")\n",
    "            print()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"üå°Ô∏è Temperature {temp}: Error - {e}\")\n",
    "            print()\n",
    "\n",
    "# Test temperature effects if models are available\n",
    "if model_names:\n",
    "    test_model = model_names[0]  # Use first available model\n",
    "    creative_prompt = \"Write a creative opening line for a story about a robot.\"\n",
    "    \n",
    "    test_temperature_effects(test_model, creative_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Model Recommendation Engine\n",
    "def recommend_model(all_results, use_case=\"general\"):\n",
    "    \"\"\"Recommend best model based on use case.\"\"\"\n",
    "    \n",
    "    if not all_results:\n",
    "        return \"No models tested\"\n",
    "    \n",
    "    use_case_weights = {\n",
    "        \"speed\": {\"avg_time\": -1.0, \"success_rate\": 0.3},  # Prioritize speed\n",
    "        \"reliability\": {\"avg_time\": -0.3, \"success_rate\": 1.0},  # Prioritize reliability\n",
    "        \"general\": {\"avg_time\": -0.6, \"success_rate\": 0.7},  # Balanced\n",
    "    }\n",
    "    \n",
    "    weights = use_case_weights.get(use_case, use_case_weights[\"general\"])\n",
    "    \n",
    "    # Calculate scores\n",
    "    scored_results = []\n",
    "    \n",
    "    for result in all_results:\n",
    "        # Normalize metrics (lower time is better, higher success rate is better)\n",
    "        max_time = max(r['avg_time'] for r in all_results)\n",
    "        min_time = min(r['avg_time'] for r in all_results)\n",
    "        \n",
    "        normalized_time = (max_time - result['avg_time']) / (max_time - min_time) if max_time != min_time else 1\n",
    "        normalized_success = result['success_rate'] / 100\n",
    "        \n",
    "        # Calculate weighted score\n",
    "        score = (normalized_time * abs(weights['avg_time']) + \n",
    "                normalized_success * weights['success_rate'])\n",
    "        \n",
    "        scored_results.append({\n",
    "            'model': result['model'],\n",
    "            'score': score,\n",
    "            'avg_time': result['avg_time'],\n",
    "            'success_rate': result['success_rate']\n",
    "        })\n",
    "    \n",
    "    # Sort by score\n",
    "    scored_results.sort(key=lambda x: x['score'], reverse=True)\n",
    "    \n",
    "    return scored_results\n",
    "\n",
    "# Generate recommendations\n",
    "if all_results:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"MODEL RECOMMENDATIONS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    use_cases = [\"speed\", \"reliability\", \"general\"]\n",
    "    \n",
    "    for use_case in use_cases:\n",
    "        recommendations = recommend_model(all_results, use_case)\n",
    "        \n",
    "        print(f\"\\nüéØ Best for {use_case}:\")\n",
    "        for i, rec in enumerate(recommendations[:2], 1):  # Top 2 recommendations\n",
    "            print(f\"  {i}. {rec['model']} (Score: {rec['score']:.2f})\")\n",
    "            print(f\"     Avg time: {rec['avg_time']:.2f}s, Success: {rec['success_rate']:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Testing Summary and Best Practices\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL TESTING SUMMARY & BEST PRACTICES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if all_results:\n",
    "    total_models_tested = len(all_results)\n",
    "    total_tests = sum(len(result['tests']) for result in all_results)\n",
    "    successful_tests = sum(sum(1 for test in result['tests'] if test['success']) for result in all_results)\n",
    "    \n",
    "    print(f\"\\nüìä Test Statistics:\")\n",
    "    print(f\"‚Ä¢ Models tested: {total_models_tested}\")\n",
    "    print(f\"‚Ä¢ Total tests run: {total_tests}\")\n",
    "    print(f\"‚Ä¢ Successful tests: {successful_tests}/{total_tests} ({(successful_tests/total_tests)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nüí° Best Practices for Model Selection:\")\n",
    "\n",
    "practices = {\n",
    "    \"üöÄ For Development\": [\n",
    "        \"Use smaller, faster models (phi, llama2:7b)\",\n",
    "        \"Prioritize response time over perfect quality\",\n",
    "        \"Test with temperature=0.1 for consistent results\"\n",
    "    ],\n",
    "    \"üéØ For Production\": [\n",
    "        \"Balance speed and quality based on use case\",\n",
    "        \"Test with realistic data volumes\",\n",
    "        \"Monitor success rates and error patterns\"\n",
    "    ],\n",
    "    \"üß™ For Experimentation\": [\n",
    "        \"Compare multiple models on same tasks\",\n",
    "        \"Test different temperature settings\",\n",
    "        \"Measure both speed and quality metrics\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "for category, tips in practices.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for tip in tips:\n",
    "        print(f\"  ‚Ä¢ {tip}\")\n",
    "\n",
    "print(f\"\\nüîß Configuration Tips:\")\n",
    "print(f\"‚Ä¢ temperature=0.0-0.3: Deterministic, good for facts\")\n",
    "print(f\"‚Ä¢ temperature=0.4-0.7: Balanced, good for general use\")\n",
    "print(f\"‚Ä¢ temperature=0.8-1.0: Creative, good for writing\")\n",
    "print(f\"‚Ä¢ max_tokens: Adjust based on expected response length\")\n",
    "\n",
    "print(f\"\\nüöÄ Next Steps:\")\n",
    "if all_results:\n",
    "    best_model = min(all_results, key=lambda x: x['avg_time'])\n",
    "    print(f\"‚Ä¢ Use {best_model['model']} for development (fastest)\")\n",
    "print(f\"‚Ä¢ Explore advanced examples with your chosen model\")\n",
    "print(f\"‚Ä¢ Next: advanced_examples/01_prompt_engineering.ipynb\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  }
 ],\n "metadata": {\n  "kernelspec": {\n   "display_name": "Python 3",\n   "language": "python",\n   "name": "python3"\n  },\n  "language_info": {\n   "codemirror_mode": {\n    "name": "ipython",\n    "version": 3\n   },\n   "file_extension": ".py",\n   "mimetype": "text/x-python",\n   "name": "python",\n   "nbconvert_exporter": "python",\n   "pygments_lexer": "ipython3",\n   "version": "3.8.5"\n  }\n },\n "nbformat": 4,\n "nbformat_minor": 4\n}