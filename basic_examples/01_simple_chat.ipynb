{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Chat with LangChain + Ollama\n",
    "\n",
    "This notebook demonstrates basic chat functionality using LangChain with local Ollama models.\n",
    "\n",
    "**What you'll learn:**\n",
    "- Basic ChatOllama setup and configuration\n",
    "- Simple chat interactions\n",
    "- Response handling and formatting\n",
    "- Error handling and troubleshooting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import sys\n",
    "sys.path.append('..')  # Add parent directory to path\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "from config import config\n",
    "\n",
    "print(\"🚀 Setting up LangChain + Ollama chat...\")\n",
    "print(f\"Using model: {config.default_model}\")\n",
    "print(f\"Ollama URL: {config.ollama_base_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Basic ChatOllama Setup\n",
    "\n",
    "# Create a ChatOllama instance with basic configuration\n",
    "llm = ChatOllama(\n",
    "    model=config.default_model,\n",
    "    base_url=config.ollama_base_url,\n",
    "    temperature=0.7,  # Controls randomness (0.0 = deterministic, 1.0 = very random)\n",
    "    max_tokens=500,   # Maximum tokens in the response\n",
    ")\n",
    "\n",
    "print(\"✅ ChatOllama instance created successfully!\")\n",
    "print(f\"Model: {llm.model}\")\n",
    "print(f\"Temperature: {llm.temperature}\")\n",
    "print(f\"Base URL: {llm.base_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Simple String Input\n",
    "print(\"💬 Testing simple string input...\")\n",
    "\n",
    "try:\n",
    "    # Most basic way to use the model\n",
    "    response = llm.invoke(\"Hello! Can you tell me a joke?\")\n",
    "    \n",
    "    print(f\"✅ Response received:\")\n",
    "    print(f\"📝 {response.content}\")\n",
    "    print(f\"📊 Response type: {type(response)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: {e}\")\n",
    "    print(\"Make sure Ollama is running and the model is available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Using Message Objects\n",
    "print(\"💬 Testing with message objects...\")\n",
    "\n",
    "# Using HumanMessage for more structured input\n",
    "try:\n",
    "    message = HumanMessage(content=\"What's the capital of France?\")\n",
    "    response = llm.invoke([message])\n",
    "    \n",
    "    print(f\"✅ Response to structured message:\")\n",
    "    print(f\"📝 {response.content}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Using System Messages for Context\n",
    "print(\"💬 Testing with system and human messages...\")\n",
    "\n",
    "try:\n",
    "    messages = [\n",
    "        SystemMessage(content=\"You are a helpful assistant that always responds in a friendly and concise manner.\"),\n",
    "        HumanMessage(content=\"Explain what Python is in one sentence.\")\n",
    "    ]\n",
    "    \n",
    "    response = llm.invoke(messages)\n",
    "    \n",
    "    print(f\"✅ Response with system context:\")\n",
    "    print(f\"📝 {response.content}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Interactive Chat Function\n",
    "def chat_with_ollama(user_input, system_prompt=None, temperature=0.7):\n",
    "    \"\"\"Simple chat function with optional system prompt.\"\"\"\n",
    "    \n",
    "    # Create a new instance with specified temperature\n",
    "    chat_llm = ChatOllama(\n",
    "        model=config.default_model,\n",
    "        base_url=config.ollama_base_url,\n",
    "        temperature=temperature,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    \n",
    "    messages = []\n",
    "    \n",
    "    # Add system message if provided\n",
    "    if system_prompt:\n",
    "        messages.append(SystemMessage(content=system_prompt))\n",
    "    \n",
    "    # Add user message\n",
    "    messages.append(HumanMessage(content=user_input))\n",
    "    \n",
    "    try:\n",
    "        response = chat_llm.invoke(messages)\n",
    "        return response.content\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "# Test the chat function\n",
    "print(\"🧪 Testing interactive chat function...\")\n",
    "\n",
    "# Example 1: Basic question\n",
    "response1 = chat_with_ollama(\"What are the main benefits of using Python?\")\n",
    "print(f\"\\n👤 What are the main benefits of using Python?\")\n",
    "print(f\"🤖 {response1}\")\n",
    "\n",
    "# Example 2: With system prompt and low temperature\n",
    "response2 = chat_with_ollama(\n",
    "    \"List 3 programming languages\",\n",
    "    system_prompt=\"You are a concise technical expert. Always provide numbered lists.\",\n",
    "    temperature=0.1\n",
    ")\n",
    "print(f\"\\n👤 List 3 programming languages (with system prompt)\")\n",
    "print(f\"🤖 {response2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Comparing Different Temperature Settings\n",
    "print(\"🌡️ Testing different temperature settings...\")\n",
    "\n",
    "question = \"Write a creative opening line for a story about a robot.\"\n",
    "temperatures = [0.0, 0.5, 1.0]\n",
    "\n",
    "for temp in temperatures:\n",
    "    print(f\"\\n🌡️ Temperature: {temp}\")\n",
    "    response = chat_with_ollama(question, temperature=temp)\n",
    "    print(f\"📝 {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Error Handling and Troubleshooting\n",
    "print(\"🔧 Testing error handling...\")\n",
    "\n",
    "# Test with a very long input to see how the model handles it\n",
    "def test_long_input():\n",
    "    long_text = \"Explain this: \" + \"This is a very long sentence. \" * 100\n",
    "    print(f\"Input length: {len(long_text)} characters\")\n",
    "    \n",
    "    try:\n",
    "        response = chat_with_ollama(long_text[:1000])  # Truncate to reasonable size\n",
    "        print(f\"✅ Long input handled successfully\")\n",
    "        print(f\"📝 Response: {response[:100]}...\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error with long input: {e}\")\n",
    "\n",
    "test_long_input()\n",
    "\n",
    "# Test connection error handling\n",
    "def test_connection_error():\n",
    "    print(\"\\n🔗 Testing connection error handling...\")\n",
    "    \n",
    "    # Create instance with invalid URL\n",
    "    try:\n",
    "        bad_llm = ChatOllama(\n",
    "            model=config.default_model,\n",
    "            base_url=\"http://localhost:99999\",  # Invalid port\n",
    "            temperature=0.7\n",
    "        )\n",
    "        \n",
    "        response = bad_llm.invoke(\"Hello\")\n",
    "        print(\"Unexpected: This should have failed\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✅ Connection error handled properly: {type(e).__name__}\")\n",
    "        print(f\"📝 Error message: {str(e)[:100]}...\")\n",
    "\n",
    "test_connection_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Quick Reference and Tips\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"QUICK REFERENCE - Simple Chat with LangChain + Ollama\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n📚 Basic Usage Patterns:\")\n",
    "print(\"\\n1. Simple string input:\")\n",
    "print('   response = llm.invoke(\"Your question here\")')\n",
    "\n",
    "print(\"\\n2. With message objects:\")\n",
    "print('   messages = [HumanMessage(content=\"Your question\")]')\n",
    "print('   response = llm.invoke(messages)')\n",
    "\n",
    "print(\"\\n3. With system context:\")\n",
    "print('   messages = [')\n",
    "print('       SystemMessage(content=\"You are a helpful assistant\"),')\n",
    "print('       HumanMessage(content=\"Your question\")')\n",
    "print('   ]')\n",
    "print('   response = llm.invoke(messages)')\n",
    "\n",
    "print(\"\\n💡 Key Parameters:\")\n",
    "print(\"• temperature: 0.0 (deterministic) to 1.0 (creative)\")\n",
    "print(\"• max_tokens: Maximum response length\")\n",
    "print(\"• model: Ollama model name (e.g., 'llama2', 'mistral')\")\n",
    "print(\"• base_url: Ollama server URL (default: http://localhost:11434)\")\n",
    "\n",
    "print(\"\\n🚨 Common Issues:\")\n",
    "print(\"• Connection error: Check if Ollama server is running\")\n",
    "print(\"• Model not found: Make sure the model is pulled (ollama pull <model>)\")\n",
    "print(\"• Slow responses: Try smaller models or lower max_tokens\")\n",
    "print(\"• Memory issues: Close other applications or use smaller models\")\n",
    "\n",
    "print(\"\\n✅ Ready for more advanced examples!\")\n",
    "print(\"Next: basic_examples/02_basic_prompts.ipynb\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],\n "metadata": {\n  "kernelspec": {\n   "display_name": "Python 3",\n   "language": "python",\n   "name": "python3"\n  },\n  "language_info": {\n   "codemirror_mode": {\n    "name": "ipython",\n    "version": 3\n   },\n   "file_extension": ".py",\n   "mimetype": "text/x-python",\n   "name": "python",\n   "nbconvert_exporter": "python",\n   "pygments_lexer": "ipython3",\n   "version": "3.8.5"\n  }\n },\n "nbformat": 4,\n "nbformat_minor": 4\n}