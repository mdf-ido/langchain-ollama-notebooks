{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ollama Configuration and Setup\n",
    "\n",
    "This notebook helps you configure and test your Ollama installation for use with LangChain.\n",
    "\n",
    "**What this notebook covers:**\n",
    "- Checking Ollama server status\n",
    "- Listing available models\n",
    "- Pulling recommended models\n",
    "- Testing model performance\n",
    "- Configuration optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import sys\n",
    "sys.path.append('..')  # Add parent directory to path\n",
    "\n",
    "from config import config\n",
    "print(f\"Using Ollama URL: {config.ollama_base_url}\")\n",
    "print(f\"Default model: {config.default_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Check Ollama Server Status\n",
    "def check_ollama_status():\n",
    "    \"\"\"Check if Ollama server is running and accessible.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(f\"{config.ollama_base_url}/api/tags\", timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            print(\"‚úÖ Ollama server is running and accessible\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"‚ùå Ollama server responded with status: {response.status_code}\")\n",
    "            return False\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(\"‚ùå Cannot connect to Ollama server\")\n",
    "        print(\"   Make sure Ollama is installed and running:\")\n",
    "        print(\"   - Install: https://ollama.ai/download\")\n",
    "        print(\"   - Start: ollama serve\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error checking Ollama status: {e}\")\n",
    "        return False\n",
    "\n",
    "ollama_running = check_ollama_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. List Available Models\n",
    "def get_available_models():\n",
    "    \"\"\"Get list of models available on the Ollama server.\"\"\"\n",
    "    if not ollama_running:\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(f\"{config.ollama_base_url}/api/tags\", timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            models = data.get('models', [])\n",
    "            return models\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting models: {e}\")\n",
    "        return []\n",
    "\n",
    "models = get_available_models()\n",
    "print(f\"Found {len(models)} installed models:\")\n",
    "\n",
    "if models:\n",
    "    for model in models:\n",
    "        name = model['name']\n",
    "        size = model.get('size', 0) / (1024**3)  # Convert to GB\n",
    "        modified = model.get('modified_at', 'Unknown')\n",
    "        print(f\"  üì¶ {name} ({size:.1f}GB) - {modified[:10] if modified != 'Unknown' else 'Unknown'}\")\n",
    "else:\n",
    "    print(\"  No models installed yet.\")\n",
    "    print(\"  Recommended models to pull:\")\n",
    "    print(\"    - llama2 (7B parameters, good for general use)\")\n",
    "    print(\"    - codellama (7B parameters, good for coding)\")\n",
    "    print(\"    - mistral (7B parameters, fast and efficient)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Model Recommendations and Pulling\n",
    "recommended_models = {\n",
    "    'llama2': {'size': '3.8GB', 'description': 'General-purpose model, good for most tasks'},\n",
    "    'mistral': {'size': '4.1GB', 'description': 'Fast and efficient, great for quick responses'},\n",
    "    'codellama': {'size': '3.8GB', 'description': 'Specialized for code generation and analysis'},\n",
    "    'llama2:13b': {'size': '7.3GB', 'description': 'Larger model, better quality but slower'},\n",
    "    'phi': {'size': '1.6GB', 'description': 'Small and fast, good for simple tasks'}\n",
    "}\n",
    "\n",
    "print(\"üìã Recommended models for LangChain development:\")\n",
    "print()\n",
    "\n",
    "installed_model_names = [model['name'].split(':')[0] for model in models]\n",
    "\n",
    "for model_name, info in recommended_models.items():\n",
    "    base_name = model_name.split(':')[0]\n",
    "    status = \"‚úÖ Installed\" if base_name in installed_model_names else \"‚¨áÔ∏è Not installed\"\n",
    "    print(f\"{status} {model_name:12} | {info['size']:>6} | {info['description']}\")\n",
    "\n",
    "print()\n",
    "print(\"To pull a model, run in terminal: ollama pull <model_name>\")\n",
    "print(\"Example: ollama pull llama2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Test Model Performance (if models are available)\n",
    "def test_model_performance(model_name, test_prompt=\"Hello! Please respond with exactly 'Test successful'\"):\n",
    "    \"\"\"Test a model's response time and functionality.\"\"\"\n",
    "    if not ollama_running:\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        print(f\"Testing {model_name}...\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Use Ollama API directly for testing\n",
    "        payload = {\n",
    "            \"model\": model_name,\n",
    "            \"prompt\": test_prompt,\n",
    "            \"stream\": False\n",
    "        }\n",
    "        \n",
    "        response = requests.post(\n",
    "            f\"{config.ollama_base_url}/api/generate\",\n",
    "            json=payload,\n",
    "            timeout=30\n",
    "        )\n",
    "        \n",
    "        end_time = time.time()\n",
    "        response_time = end_time - start_time\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            response_text = result.get('response', 'No response')\n",
    "            print(f\"  ‚úÖ Response time: {response_time:.2f}s\")\n",
    "            print(f\"  üìù Response: {response_text[:100]}{'...' if len(response_text) > 100 else ''}\")\n",
    "            return {\n",
    "                'model': model_name,\n",
    "                'response_time': response_time,\n",
    "                'response': response_text,\n",
    "                'success': True\n",
    "            }\n",
    "        else:\n",
    "            print(f\"  ‚ùå API error: {response.status_code}\")\n",
    "            return None\n",
    "            \n",
    "    except requests.exceptions.Timeout:\n",
    "        print(f\"  ‚è±Ô∏è Request timed out (>30s)\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test available models\n",
    "if models:\n",
    "    print(\"üß™ Testing model performance...\")\n",
    "    print()\n",
    "    \n",
    "    test_results = []\n",
    "    \n",
    "    # Test up to 3 models to avoid long execution times\n",
    "    models_to_test = models[:3]\n",
    "    \n",
    "    for model in models_to_test:\n",
    "        model_name = model['name']\n",
    "        result = test_model_performance(model_name)\n",
    "        if result:\n",
    "            test_results.append(result)\n",
    "        print()\n",
    "    \n",
    "    # Summary of results\n",
    "    if test_results:\n",
    "        print(\"üìä Performance Summary:\")\n",
    "        test_results.sort(key=lambda x: x['response_time'])\n",
    "        for i, result in enumerate(test_results, 1):\n",
    "            print(f\"  {i}. {result['model']}: {result['response_time']:.2f}s\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No models available for testing. Please install a model first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. LangChain Integration Test\n",
    "def test_langchain_integration():\n",
    "    \"\"\"Test LangChain integration with available models.\"\"\"\n",
    "    if not models:\n",
    "        print(\"‚ö†Ô∏è No models available for LangChain testing\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        from langchain_ollama import ChatOllama\n",
    "        \n",
    "        # Test with the first available model\n",
    "        test_model = models[0]['name']\n",
    "        print(f\"Testing LangChain integration with {test_model}...\")\n",
    "        \n",
    "        llm = ChatOllama(\n",
    "            model=test_model,\n",
    "            base_url=config.ollama_base_url,\n",
    "            temperature=0.1\n",
    "        )\n",
    "        \n",
    "        response = llm.invoke(\"What is 2+2? Answer with just the number.\")\n",
    "        print(f\"‚úÖ LangChain integration successful\")\n",
    "        print(f\"üìù Test response: {response.content}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"‚ùå LangChain not properly installed\")\n",
    "        print(\"   Install with: pip install langchain langchain-ollama\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå LangChain integration failed: {e}\")\n",
    "        return False\n",
    "\n",
    "langchain_ok = test_langchain_integration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Configuration Summary and Recommendations\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"OLLAMA CONFIGURATION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"Server Status: {'‚úÖ Running' if ollama_running else '‚ùå Not accessible'}\")\n",
    "print(f\"Models Installed: {len(models)} models\")\n",
    "print(f\"LangChain Integration: {'‚úÖ Working' if langchain_ok else '‚ùå Issues found'}\")\n",
    "print(f\"Default Model: {config.default_model}\")\n",
    "\n",
    "if models:\n",
    "    fastest_model = min(test_results, key=lambda x: x['response_time'])['model'] if 'test_results' in locals() and test_results else models[0]['name']\n",
    "    print(f\"Fastest Model: {fastest_model}\")\n",
    "\n",
    "print(\"\\nüìã Next Steps:\")\n",
    "\n",
    "if not ollama_running:\n",
    "    print(\"1. ‚ùå Install and start Ollama server\")\n",
    "    print(\"   Visit: https://ollama.ai/download\")\n",
    "elif not models:\n",
    "    print(\"1. ‚¨áÔ∏è Install at least one model:\")\n",
    "    print(\"   ollama pull llama2\")\n",
    "elif not langchain_ok:\n",
    "    print(\"1. üîß Fix LangChain installation:\")\n",
    "    print(\"   pip install langchain langchain-ollama\")\n",
    "else:\n",
    "    print(\"1. ‚úÖ Ready to use! Try the basic examples.\")\n",
    "    print(\"2. üìñ Check out: basic_examples/01_simple_chat.ipynb\")\n",
    "\n",
    "print(\"\\nüí° Performance Tips:\")\n",
    "print(\"- Smaller models (phi, llama2) are faster for development\")\n",
    "print(\"- Use temperature=0.1 for consistent testing\")\n",
    "print(\"- Keep Ollama server running in the background\")\n",
    "print(\"- Monitor system RAM usage with larger models\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  }
 ],\n "metadata": {\n  "kernelspec": {\n   "display_name": "Python 3",\n   "language": "python",\n   "name": "python3"\n  },\n  "language_info": {\n   "codemirror_mode": {\n    "name": "ipython",\n    "version": 3\n   },\n   "file_extension": ".py",\n   "mimetype": "text/x-python",\n   "name": "python",\n   "nbconvert_exporter": "python",\n   "pygments_lexer": "ipython3",\n   "version": "3.8.5"\n  }\n },\n "nbformat": 4,\n "nbformat_minor": 4\n}