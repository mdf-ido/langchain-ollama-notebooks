{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ollama Configuration and Setup\n\nThis notebook helps you configure and test your Ollama installation for use with LangChain.\n\n**What this notebook covers:**\n- Verify Ollama installation and connectivity\n- Test model availability\n- Configure connection settings\n- Validate the setup for LangChain integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\nimport sys\nsys.path.append('..')  # Add parent directory to path\n\nfrom config import config\nimport requests\nimport json\n\nprint(f\"Using Ollama URL: {config.ollama_base_url}\")\nprint(f\"Default model: {config.default_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Ollama connectivity\ndef check_ollama_status():\n    \"\"\"Check if Ollama server is running and accessible.\"\"\"\n    try:\n        response = requests.get(f\"{config.ollama_base_url}\")\n        if response.status_code == 200:\n            print(\"‚úÖ Ollama server is running and accessible!\")\n            return True\n        else:\n            print(f\"‚ùå Ollama server responded with status: {response.status_code}\")\n            return False\n    except requests.exceptions.ConnectionError:\n        print(\"‚ùå Cannot connect to Ollama server!\")\n        print(\"   Make sure Ollama is installed and running:\")\n        print(\"   - Install: https://ollama.ai/download\")\n        print(\"   - Start: ollama serve\")\n        return False\n    except Exception as e:\n        print(f\"‚ùå Error checking Ollama status: {e}\")\n        return False\n\nollama_running = check_ollama_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of available models\nmodels = []\nif ollama_running:\n    print(\"",
    "üîç Checking available models...\")\n    \n    try:\n        response = requests.get(f\"{config.ollama_base_url}/api/tags\")\n        if response.status_code == 200:\n            data = response.json()\n            models = [model['name'] for model in data.get('models', [])]\n        else:\n            print(f\"Error getting model list: {response.status_code}\")\n    except Exception as e:\n        print(f\"Error getting models: {e}\")\n\nprint(f\"",
    "Found {len(models)} installed models:\")\nif models:\n    for model in models:\n        print(f\"  - {model}\")\nelse:\n    print(\"  No models found. You may need to pull some models first.\")\n    print(\"  Example: ollama pull llama2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test default model\nif config.default_model in models:\n    print(f\"",
    "‚úÖ Default model '{config.default_model}' is available!\")\nelif models:\n    print(f\"",
    "‚ö†Ô∏è  Default model '{config.default_model}' not found.\")\n    print(\"   Available models:\")\n    for model in models:\n        print(f\"     - {model}\")\n    print(f\"",
    "   Consider updating config.py or pulling the model:\")\n    print(f\"   ollama pull {config.default_model}\")\nelse:\n    print(f\"",
    "‚ùå No models available. Please pull some models first:\")\n    print(f\"   ollama pull {config.default_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test basic model interaction (if available)\nif models and config.default_model in models:\n    print(f\"",
    "üß™ Testing basic interaction with {config.default_model}...\")\n    \n    try:\n        test_payload = {\n            \"model\": config.default_model,\n            \"prompt\": \"Hello! Please respond with just 'Test successful'\",\n            \"stream\": False\n        }\n        \n        response = requests.post(\n            f\"{config.ollama_base_url}/api/generate\",\n            json=test_payload,\n            timeout=30\n        )\n        \n        if response.status_code == 200:\n            result = response.json()\n            print(\"‚úÖ Model test successful!\")\n            print(f\"   Response: {result.get('response', 'No response')[:100]}...\")\n        else:\n            print(f\"‚ùå Model test failed with status: {response.status_code}\")\n            \n    except Exception as e:\n        print(f\"‚ùå Model test failed: {e}\")\nelse:\n    print(\"",
    "‚ö†Ô∏è  Skipping model test (no compatible models available)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n\nIf all tests above passed, you're ready to use Ollama with LangChain!\n\n**Next steps:**\n1. If tests failed, check the Ollama installation and make sure it's running\n2. Pull required models: `ollama pull llama2` (or your preferred model)  \n3. Update `config.py` if needed\n4. Proceed to the example notebooks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}