# LangChain Ollama Notebooks

A comprehensive collection of Jupyter notebooks for testing and exploring LangChain with local Ollama GPT-OSS models. Designed for use with JupyterHub environments.

## ğŸ¯ Overview

This repository provides practical examples and templates for:
- **LangChain integration** with Ollama models
- **Prompt engineering** techniques and best practices
- **Advanced workflows** including RAG, agents, and chains
- **Performance testing** and model comparison
- **Real-world use cases** and applications

## ğŸ“‹ Prerequisites

- **Python 3.8+**
- **Ollama installed locally** with at least one model downloaded
- **JupyterHub server** (you mentioned you already have this)
- Basic knowledge of Python and Jupyter notebooks

## ğŸš€ Quick Start

1. **Clone the repository:**
   ```bash
   git clone https://github.com/mdf-ido/langchain-ollama-notebooks.git
   cd langchain-ollama-notebooks
   ```

2. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

3. **Set up environment:**
   ```bash
   cp .env.example .env
   # Edit .env with your Ollama settings
   ```

4. **Start with setup notebooks:**
   - Open `setup/00_environment_setup.ipynb`
   - Then `setup/01_ollama_configuration.ipynb`

## ğŸ“ Repository Structure

```
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â”œâ”€â”€ config.py
â”œâ”€â”€ setup/
â”‚   â”œâ”€â”€ 00_environment_setup.ipynb
â”‚   â””â”€â”€ 01_ollama_configuration.ipynb
â”œâ”€â”€ basic_examples/
â”‚   â”œâ”€â”€ 01_simple_chat.ipynb
â”‚   â”œâ”€â”€ 02_basic_prompts.ipynb
â”‚   â””â”€â”€ 03_model_testing.ipynb
â”œâ”€â”€ advanced_examples/
â”‚   â”œâ”€â”€ 01_prompt_engineering.ipynb
â”‚   â”œâ”€â”€ 02_chains_and_workflows.ipynb
â”‚   â”œâ”€â”€ 03_memory_and_conversation.ipynb
â”‚   â”œâ”€â”€ 04_rag_document_processing.ipynb
â”‚   â””â”€â”€ 05_agents_and_tools.ipynb
â”œâ”€â”€ utilities/
â”‚   â”œâ”€â”€ model_comparison.ipynb
â”‚   â”œâ”€â”€ performance_testing.ipynb
â”‚   â””â”€â”€ debugging_tools.ipynb
â””â”€â”€ docs/
    â”œâ”€â”€ best_practices.md
    â””â”€â”€ troubleshooting.md
```

## ğŸ”§ Configuration

1. **Ollama Setup:** Ensure Ollama is running locally on `http://localhost:11434`
2. **Models:** Download your preferred models (e.g., `ollama pull llama2`, `ollama pull codellama`)
3. **Environment:** Copy `.env.example` to `.env` and configure your settings

## ğŸ“š Usage Examples

### Basic Chat
```python
from langchain_ollama import ChatOllama
llm = ChatOllama(model="llama2", base_url="http://localhost:11434")
response = llm.invoke("Hello, how are you?")
```

### Prompt Templates
```python
from langchain.prompts import ChatPromptTemplate
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful AI assistant."),
    ("human", "{input}")
])
```

## ğŸ› ï¸ Troubleshooting

- **Ollama not responding:** Check if Ollama service is running
- **Model not found:** Ensure the model is pulled locally
- **Memory issues:** Consider using smaller models for testing
- **JupyterHub compatibility:** All notebooks tested with standard JupyterHub setups

## ğŸ¤ Contributing

Feel free to submit issues, fork the repository, and create pull requests for improvements!

## ğŸ“„ License

MIT License - feel free to use and modify as needed.