# Multi-Framework AI Ollama Notebooks

A comprehensive collection of Jupyter notebooks for testing and exploring multiple AI frameworks including **LangChain**, **Microsoft Semantic Kernel**, **PydanticAI**, **CrewAI**, and **DSPy** with self-hosted Ollama models. All notebooks are in standard JSON `.ipynb` format for full compatibility with JupyterHub and other Jupyter environments.

## ğŸ¯ Overview

This repository provides practical examples and templates for:
- **LangChain integration** with self-hosted Ollama models
- **Microsoft Semantic Kernel** integration with Ollama 
- **PydanticAI** for type-safe AI agent development
- **CrewAI** for advanced multi-agent workflows (coming soon)
- **DSPy** for data science and AI pipeline optimization (coming soon)
- **Plugin architecture** and kernel function development
- **AI Learning Tutor** system with comprehensive educational features
- **Prompt engineering** techniques and best practices
- **Advanced workflows** including RAG, agents, and chains
- **Performance testing** and model comparison across frameworks
- **Real-world use cases** and applications

## ğŸ—ï¸ Self-Hosted Architecture

This setup is designed for **completely self-hosted AI workflows**:
- **Local Ollama server** - Run models on your own hardware
- **Standard Jupyter notebooks** - All `.ipynb` files use JSON format for universal compatibility
- **JupyterHub ready** - Works seamlessly with JupyterHub deployments
- **No external API dependencies** - Everything runs on your infrastructure
- **Privacy-first** - All data stays on your servers

## ğŸ“‹ Prerequisites

- **Python 3.8+**
- **Self-hosted Ollama server** with at least one model downloaded
- **JupyterHub or Jupyter environment** (notebooks use standard JSON format)
- **Network access** to your Ollama server (local or remote)
- Basic knowledge of Python and Jupyter notebooks

## ğŸš€ Quick Start

1. **Clone the repository:**
   ```bash
   git clone https://github.com/mdf-ido/langchain-ollama-notebooks.git
   cd langchain-ollama-notebooks
   ```

2. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```
   *Note: Includes LangChain, Semantic Kernel, PydanticAI, and other AI framework dependencies*

3. **Set up environment:**
   ```bash
   cp .env.example .env
   # Edit .env with your Ollama settings
   ```

4. **Start with setup notebooks:**
   - Open `setup/00_environment_setup.ipynb`
   - Then `setup/01_ollama_configuration.ipynb`
   - Try LangChain examples in `langchain/` folder
   - Explore Semantic Kernel examples in `semantic_kernel/` folder
   - Test PydanticAI structured agents in `PydanticAI/` folder
   - Future: CrewAI and DSPy examples (coming soon)

## ğŸ“ Repository Structure

```
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt           # Multi-framework AI dependencies
â”œâ”€â”€ config.py                 # Shared configuration for all frameworks  
â”œâ”€â”€ setup/
â”‚   â”œâ”€â”€ 00_environment_setup.ipynb
â”‚   â””â”€â”€ 01_ollama_configuration.ipynb
â”œâ”€â”€ langchain/                # LangChain Examples
â”‚   â”œâ”€â”€ 01_simple_chat.ipynb
â”‚   â”œâ”€â”€ 01_simple_chat_clean.ipynb
â”‚   â”œâ”€â”€ 01_simple_chat_minimal.ipynb
â”‚   â”œâ”€â”€ 02_basic_prompts.ipynb
â”‚   â”œâ”€â”€ 03_model_testing.ipynb
â”‚   â””â”€â”€ 01_prompt_engineering.ipynb
â”œâ”€â”€ semantic_kernel/          # Microsoft Semantic Kernel Examples
â”‚   â”œâ”€â”€ 01_simple_chat_sk.ipynb
â”‚   â””â”€â”€ 02_ai_learning_tutor_sk.ipynb
â”œâ”€â”€ PydanticAI/              # PydanticAI Framework Examples
â”‚   â””â”€â”€ 01_simple_chat_pydantic_ai.ipynb
â”œâ”€â”€ crewAi/                  # CrewAI Multi-Agent Examples (coming soon)
â””â”€â”€ DSpy/                    # DSPy Data Science Examples (coming soon)
```

### ğŸ§  Semantic Kernel Features

The `semantic_kernel/` folder includes advanced AI orchestration examples:

**01_simple_chat_sk.ipynb**
- Basic Semantic Kernel setup with Ollama
- Plugin architecture demonstration
- Chat history management
- Kernel function examples

**02_ai_learning_tutor_sk.ipynb** 
- Comprehensive AI Learning Tutor system
- 5 specialized tutoring functions:
  - `find_facts` - Discover related facts and context
  - `create_examples` - Generate examples and analogies  
  - `build_mnemonics` - Create memory aids and mnemonics
  - `explore_contradictions` - Present alternative perspectives
  - `comprehensive_tutor` - All-in-one learning expansion
- Educational workflow automation
- Critical thinking enhancement tools

### ğŸ¯ PydanticAI Features

The `PydanticAI/` folder demonstrates type-safe AI agent development:

**01_simple_chat_pydantic_ai.ipynb**
- Type-safe agent creation with Pydantic models
- Structured response validation
- Dependency injection patterns
- Integration with Ollama models
- Async/await chat interactions

## ğŸ”§ Configuration

1. **Self-Hosted Ollama Setup:** 
   - Can run locally on `http://localhost:11434` 
   - Or on remote server (e.g., `http://192.168.1.81:11434`)
   - Configure in `config.py` for all supported frameworks
2. **Models:** Download your preferred models (e.g., `ollama pull llama2`, `ollama pull gpt-oss`)
3. **JupyterHub Compatibility:** All notebooks use standard JSON `.ipynb` format for seamless deployment

## ğŸ’¡ Framework Comparison

| Feature | LangChain | Semantic Kernel | PydanticAI |
|---------|-----------|----------------|------------|
| **Architecture** | Chain-based workflows | Plugin-based kernel functions | Type-safe agents |
| **Memory Management** | Manual message arrays | Built-in ChatHistory class | Agent state management |
| **Function Calling** | Direct LLM invocation | @kernel_function decorators | Structured tool integration |
| **Type Safety** | Limited | Manual typing | Native Pydantic validation |
| **Structured Output** | Manual parsing | Custom implementation | Built-in Pydantic models |
| **Ecosystem** | Extensive community tools | Microsoft enterprise focus | Modern Python patterns |
| **Use Case** | Rapid prototyping, chains | Structured AI applications | Production-grade agents |

## ğŸ“š Usage Examples

### LangChain Basic Chat
```python
from langchain_ollama import ChatOllama
llm = ChatOllama(model="gpt-oss", base_url="http://localhost:11434")
response = llm.invoke("Hello, how are you?")
```

### Semantic Kernel Setup
```python
import semantic_kernel as sk
from semantic_kernel.connectors.ai.ollama import OllamaChatCompletion

kernel = sk.Kernel()
ollama_service = OllamaChatCompletion(
    ai_model_id="gpt-oss",
    host="http://localhost:11434",
    service_id="ollama_chat"
)
kernel.add_service(ollama_service)
```

### PydanticAI Type-Safe Agents
```python
from pydantic_ai import Agent
from pydantic_ai.models import OllamaModel
from pydantic import BaseModel

class ChatResponse(BaseModel):
    message: str
    confidence: float

model = OllamaModel(model_name="gpt-oss", base_url="http://localhost:11434")
agent = Agent(model=model, result_type=ChatResponse)
response = await agent.run("Hello, how are you?")
```

### AI Learning Tutor Example
```python
# Using Semantic Kernel's AI Learning Tutor
insight = "Spaced repetition improves long-term memory"
source = "Cognitive psychology textbook"

# Get comprehensive learning expansion
tutor_function = kernel.get_function("LearningTutor", "comprehensive_tutor")
result = await tutor_function.invoke(kernel, KernelArguments(
    insight=insight, source=source
))
```

## ğŸ› ï¸ Troubleshooting

- **Ollama not responding:** Check if Ollama service is running on your server
- **Model not found:** Ensure the model is pulled locally (`ollama pull model-name`)
- **Memory issues:** Consider using smaller models for testing
- **JupyterHub compatibility:** All notebooks use standard JSON `.ipynb` format - fully compatible
- **Network issues:** For remote Ollama servers, check firewall and network policies
- **Semantic Kernel errors:** Ensure `host` parameter is used instead of `base_url`

## ğŸ¤ Contributing

Feel free to submit issues, fork the repository, and create pull requests for improvements!

## ğŸ“„ License

MIT License - feel free to use and modify as needed.